{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI-Junction/Titanic/blob/master/Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jN8Vbj7qnwLz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Introduction\n",
        "\n",
        "This notebook is a very basic and simple introductory primer to the method of ensembling (combining) \n",
        "base learning models, in particular the variant of ensembling known as Stacking. \n",
        "In a nutshell stacking uses as a first-level (base), the predictions of a few basic \n",
        "classifiers and then uses another model at the second-level to predict the output \n",
        "from the earlier first-level predictions.\n",
        "\n",
        "The Titanic dataset is a prime candidate for introducing this concept as \n",
        "many newcomers start out here. Furthermore even though stacking has been \n",
        "responsible for many there seems to be a dearth of kernels on this topic \n",
        "so I hope this notebook can fill somewhat of that void.\n",
        "'''\n"
      ]
    },
    {
      "metadata": {
        "id": "VTxujb8zniPl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#%%\n",
        "\n",
        "#load generic packages\n",
        "\n",
        "import sys #access to system parameters https://docs.python.org/3/library/sys.html\n",
        "print(\"Python version: {}\". format(sys.version))\n",
        "\n",
        "import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
        "print(\"pandas version: {}\". format(pd.__version__))\n",
        "\n",
        "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
        "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
        "\n",
        "import numpy as np #foundational package for scientific computing\n",
        "print(\"NumPy version: {}\". format(np.__version__))\n",
        "\n",
        "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
        "print(\"SciPy version: {}\". format(sp.__version__)) \n",
        "\n",
        "import IPython\n",
        "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
        "print(\"IPython version: {}\". format(IPython.__version__)) \n",
        "\n",
        "import sklearn #collection of machine learning algorithms\n",
        "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
        "\n",
        "#misc libraries\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print('-'*25)\n",
        "\n",
        "\n",
        "\n",
        "# Load in our libraries\n",
        "\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#import plotly.offline as py\n",
        "#py.init_notebook_mode(connected=True)\n",
        "#import plotly.graph_objs as go\n",
        "#import plotly.tools as tls\n",
        "#import warnings\n",
        "#import xgboost as xgb\n",
        "#import seaborn as sns\n",
        "#%matplotlib inline\n",
        "\n",
        "\n",
        "#%%\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e4JvQslEo7tz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load classification models\n",
        "\n",
        "# Going to use these 5 base models for the stacking\n",
        "\n",
        "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier)\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cross_validation import KFold\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d1L4e152pAbe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#%%\n",
        "\n",
        "# load data from local files\n",
        "\n",
        "file_path_train = \"C:\\\\Users\\\\echtpar\\\\Anaconda3\\\\KerasProjects\\\\Keras-CNN-Tutorial\\\\2018 09 28 ML Workshop CMR IT\\\\All ML Datasets\\\\titanic_train.csv\"\n",
        "file_path_test = \"C:\\\\Users\\\\echtpar\\\\Anaconda3\\\\KerasProjects\\\\Keras-CNN-Tutorial\\\\2018 09 28 ML Workshop CMR IT\\\\All ML Datasets\\\\titanic_test.csv\"\n",
        "\n",
        "train = pd.read_csv(file_path_train)\n",
        "test = pd.read_csv(file_path_test)\n",
        "\n",
        "train.shape\n",
        "test.shape\n",
        "\n",
        "train.columns.values\n",
        "test.columns\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gVPOM3e-pGVA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Which features are mixed data types?\n",
        "\n",
        "\n",
        "train.info()\n",
        "test.info()\n",
        "\n",
        "train.describe()\n",
        "test.describe()\n",
        "\n",
        "train.describe(include=[np.object])\n",
        "train.describe(include=[np.number])\n",
        "#train.describe(include=['Ticket'])\n",
        "\n",
        "\n",
        "train['Fare'].isnull().sum()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ef1QnUBypLro",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#%%\n",
        "\n",
        "'''\n",
        "Assumtions based on data analysis\n",
        "\n",
        "We arrive at following assumptions based on data analysis done so far. We \n",
        "may validate these assumptions further before taking appropriate actions.\n",
        "\n",
        "Correlating.\n",
        "\n",
        "We want to know how well does each feature correlate with Survival. \n",
        "We want to do this early in our project and match these quick correlations with \n",
        "modelled correlations later in the project.\n",
        "\n",
        "Completing.\n",
        "\n",
        "    We may want to complete Age feature as it is definitely correlated to survival.\n",
        "    We may want to complete the Embarked feature as it may also correlate with survival or \n",
        "    another important feature.\n",
        "\n",
        "Correcting.\n",
        "\n",
        "    Ticket feature may be dropped from our analysis as it contains high ratio of duplicates \n",
        "    (22%) and there may not be a correlation between Ticket and survival.\n",
        "    Cabin feature may be dropped as it is highly incomplete or contains many null values \n",
        "    both in training and test dataset.\n",
        "    PassengerId may be dropped from training dataset as it does not contribute to survival.\n",
        "    Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.\n",
        "\n",
        "Creating.\n",
        "\n",
        "    We may want to create a new feature called Family based on Parch and SibSp \n",
        "    to get total count of family members on board.\n",
        "    We may want to engineer the Name feature to extract Title as a new feature.\n",
        "    We may want to create new feature for Age bands. This turns a continous numerical \n",
        "    feature into an ordinal categorical feature.\n",
        "    We may also want to create a Fare range feature if it helps our analysis.\n",
        "\n",
        "Classifying.\n",
        "\n",
        "We may also add to our assumptions based on the problem description noted earlier.\n",
        "\n",
        "    Women (Sex=female) were more likely to have survived.\n",
        "    Children (Age<?) were more likely to have survived.\n",
        "    The upper-class passengers (Pclass=1) were more likely to have survived.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "'''\n",
        "Analyze by pivoting features\n",
        "\n",
        "To confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.\n",
        "\n",
        "    Pclass We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.\n",
        "    Sex We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).\n",
        "    SibSp and Parch These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1).\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
        "train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
        "train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
        "train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EXArVCPHpf4c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#%%\n",
        "\n",
        "'''\n",
        "Feature Exploration, Engineering and Cleaning\n",
        "\n",
        "Now we will proceed much like how most kernels in general are structured, \n",
        "and that is to first explore the data on hand, identify possible feature \n",
        "engineering opportunities as well as numerically encode any categorical features.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "# Store our passenger ID for easy access\n",
        "\n",
        "PassengerId = test['PassengerId']\n",
        "\n",
        "train.head(3)\n",
        "\n",
        "full_data = [train, test]\n",
        "\n",
        "# Some features of my own that I have added in\n",
        "\n",
        "# Gives the length of the name\n",
        "\n",
        "train['Name_length'] = train['Name'].apply(len)\n",
        "test['Name_length'] = test['Name'].apply(len)\n",
        "\n",
        "# Feature that tells whether a passenger had a cabin on the Titanic\n",
        "\n",
        "train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
        "test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
        "\n",
        "# Feature engineering steps taken from Sina\n",
        "\n",
        "# Create new feature FamilySize as a combination of SibSp and Parch\n",
        "\n",
        "for dataset in full_data:\n",
        "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
        "\n",
        "# Create new feature IsAlone from FamilySize\n",
        "\n",
        "for dataset in full_data:\n",
        "    dataset['IsAlone'] = 0\n",
        "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
        "\n",
        "# Remove all NULLS in the Embarked column\n",
        "\n",
        "for dataset in full_data:\n",
        "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
        "\n",
        "# Remove all NULLS in the Fare column and create a new feature CategoricalFare\n",
        "\n",
        "for dataset in full_data:\n",
        "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DM9X-yYSpnAv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#%%\n",
        "\n",
        "#train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n",
        "\n",
        "#train['CategoricalFare'][:3]\n",
        "\n",
        "# Create a New feature CategoricalAge\n",
        "\n",
        "for dataset in full_data:\n",
        "    age_avg = dataset['Age'].mean()\n",
        "    age_std = dataset['Age'].std()\n",
        "    age_null_count = dataset['Age'].isnull().sum()\n",
        "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
        "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
        "    dataset['Age'] = dataset['Age'].astype(int)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x7dz9P8upq3B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train['CategoricalAge'] = pd.cut(train['Age'], 5)\n",
        "\n",
        "print(train['CategoricalAge'][:5])\n",
        "\n",
        "# Define function to extract titles from passenger names\n",
        "\n",
        "def get_title(name):\n",
        "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
        "\n",
        "    # If the title exists, extract and return it.\n",
        "\n",
        "    if title_search:\n",
        "        return title_search.group(1)\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "# Create a new feature Title, containing the titles of passenger names\n",
        "\n",
        "for dataset in full_data:\n",
        "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
        "\n",
        "print(train['Title'].unique())\n",
        "\n",
        "# Group all non-common titles into one single grouping \"Rare\"\n",
        "\n",
        "for dataset in full_data:\n",
        "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s0q5NAjTpyYe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for dataset in full_data:\n",
        "    # Mapping Sex\n",
        "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
        "\n",
        "    # Mapping titles\n",
        "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
        "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
        "    dataset['Title'] = dataset['Title'].fillna(0)\n",
        "\n",
        "    # Mapping Embarked\n",
        "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
        "\n",
        "    # Mapping Fare\n",
        "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare']                               = 0\n",
        "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
        "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
        "    dataset.loc[ dataset['Fare'] > 31, 'Fare']                                  = 3\n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
        "\n",
        "    # Mapping Age\n",
        "    dataset.loc[ dataset['Age'] <= 16, 'Age']                          = 0\n",
        "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
        "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
        "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
        "    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sulIYMUfp2-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Feature selection\n",
        "\n",
        "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
        "train = train.drop(drop_elements, axis = 1)\n",
        "train = train.drop(['CategoricalAge'], axis = 1)\n",
        "test  = test.drop(drop_elements, axis = 1)\n",
        "\n",
        "train.shape\n",
        "test.shape\n",
        "train.columns\n",
        "test.columns\n",
        "\n",
        "train.head()\n",
        "\n",
        "train.dtypes\n",
        "test.dtypes\n",
        "\n",
        "for col in train.columns:\n",
        "    print(train[col].dtype)\n",
        "\n",
        "train_columns_dtype = train.dtypes.reset_index()\n",
        "train_columns_dtype.columns = ['col', 'type']\n",
        "train_columns_dtype_grouped = train_columns_dtype.groupby('type').aggregate('count').reset_index()\n",
        "print(train_columns_dtype_grouped)\n",
        "\n",
        "train.shape\n",
        "test.shape\n",
        "\n",
        "print(train.columns)\n",
        "print(test.columns)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oFKHJWxUp7v3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#%%\n",
        "\n",
        "#Compare survival rates\n",
        "\n",
        "pd.crosstab(train['Title'], train['Sex'])\n",
        "train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()\n",
        "train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
        "\n",
        "train['Fare'].isnull().sum()\n",
        "\n",
        "\n",
        "print(train.columns.values)\n",
        "\n",
        "for x in train.columns.values:\n",
        "    if train[x].dtype != 'float64' :\n",
        "        if x != 'Survived' :\n",
        "            print('Survival Correlation by:', x)\n",
        "            print(train[[x, 'Survived']].groupby(x, as_index=False).mean())\n",
        "            print('-'*10, '\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JYh-3tOwqAjI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=[16,12])\n",
        "\n",
        "plt.subplot(231)\n",
        "plt.boxplot(x=train['Fare'], showmeans = True, meanline = True)\n",
        "plt.title('Fare Boxplot')\n",
        "plt.ylabel('Fare ($)')\n",
        "\n",
        "plt.subplot(232)\n",
        "plt.boxplot(train['Age'], showmeans = True, meanline = True)\n",
        "plt.title('Age Boxplot')\n",
        "plt.ylabel('Age (Years)')\n",
        "\n",
        "plt.subplot(233)\n",
        "plt.boxplot(train['FamilySize'], showmeans = True, meanline = True)\n",
        "plt.title('Family Size Boxplot')\n",
        "plt.ylabel('Family Size (#)')\n",
        "\n",
        "plt.subplot(234)\n",
        "plt.hist(x = [train[train['Survived']==1]['Fare'], train[train['Survived']==0]['Fare']], \n",
        "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
        "plt.title('Fare Histogram by Survival')\n",
        "plt.xlabel('Fare ($)')\n",
        "plt.ylabel('# of Passengers')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(235)\n",
        "plt.hist(x = [train[train['Survived']==1]['Age'], train[train['Survived']==0]['Age']], \n",
        "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
        "plt.title('Age Histogram by Survival')\n",
        "plt.xlabel('Age (Years)')\n",
        "plt.ylabel('# of Passengers')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(236)\n",
        "plt.hist(x = [train[train['Survived']==1]['FamilySize'], train[train['Survived']==0]['FamilySize']], \n",
        "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
        "plt.title('Family Size Histogram by Survival')\n",
        "plt.xlabel('Family Size (#)')\n",
        "plt.ylabel('# of Passengers')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wiF_4N_oqEol",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\n",
        "Visualisations\n",
        "'''\n",
        "\n",
        "train.head(3)\n",
        "\n",
        "train.astype(float).corr()\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Pearson Correlation Heatmap\n",
        "\n",
        "\n",
        "\n",
        "let us generate some correlation plots of the features to see how \n",
        "related one feature is to the next. To do so, we will utilise the \n",
        "Seaborn plotting package which allows us to plot heatmaps very \n",
        "conveniently as follows\n",
        "'''\n",
        "\n",
        "colormap = plt.cm.RdBu\n",
        "plt.figure(figsize=(14,12))\n",
        "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
        "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n",
        "            square=True, cmap=colormap, linecolor='white', annot=True)\n",
        "\n",
        "'''\n",
        "Takeaway from the Plots\n",
        "\n",
        "One thing that that the Pearson Correlation plot can tell us is that \n",
        "there are not too many features strongly correlated with one another. \n",
        "This is good from a point of view of feeding these features into your \n",
        "learning model because this means that there isn't much redundant or \n",
        "superfluous data in our training set and we are happy that each feature \n",
        "carries with it some unique information. Here are two most correlated \n",
        "features are that of Family size and Parch (Parents and Children). I'll \n",
        "still leave both features in for the purposes of this exercise.\n",
        "\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DeH4BE76qIyF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nQp1EV5fqMob",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "X_train = train.drop(\"Survived\", axis=1)\n",
        "Y_train = train[\"Survived\"]\n",
        "X_test = test\n",
        "\n",
        "X_train.shape, Y_train.shape, X_test.shape\n",
        "\n",
        "X_train.columns\n",
        "X_test.columns\n",
        "\n",
        "\n",
        "# Logistic Regression\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, Y_train)\n",
        "Y_pred = logreg.predict(X_test)\n",
        "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
        "acc_log\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dN04GFB9qRT2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "Next we model using Support Vector Machines which are supervised learning models \n",
        "with associated learning algorithms that analyze data used for classification and \n",
        "regression analysis. Given a set of training samples, each marked as belonging to \n",
        "one or the other of two categories, an SVM training algorithm builds a model that \n",
        "assigns new test samples to one category or the other, making it a \n",
        "non-probabilistic binary linear classifier. Reference Wikipedia.\n",
        "\n",
        "Note that the model generates a confidence score which is higher than \n",
        "Logistics Regression model.\n",
        "'''\n",
        "\n",
        "\n",
        "# Support Vector Machines\n",
        "\n",
        "svc = SVC()\n",
        "svc.fit(X_train, Y_train)\n",
        "Y_pred = svc.predict(X_test)\n",
        "acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n",
        "acc_svc\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "51XutBYKqVRD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN \n",
        "for short) is a non-parametric method used for classification \n",
        "and regression. A sample is classified by a majority vote of \n",
        "its neighbors, with the sample being assigned to the class \n",
        "most common among its k nearest neighbors (k is a positive \n",
        "integer, typically small). If k = 1, then the object is \n",
        "simply assigned to the class of that single nearest neighbor. \n",
        "Reference Wikipedia.\n",
        "\n",
        "KNN confidence score is better than Logistics Regression \n",
        "but worse than SVM.\n",
        "'''\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors = 3)\n",
        "knn.fit(X_train, Y_train)\n",
        "Y_pred = knn.predict(X_test)\n",
        "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
        "acc_knn\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "61YwY_eBqbUc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "In machine learning, naive Bayes classifiers are a family of \n",
        "simple probabilistic classifiers based on applying Bayes' theorem with \n",
        "strong (naive) independence assumptions between the features. \n",
        "Naive Bayes classifiers are highly scalable, requiring a \n",
        "number of parameters linear in the number of variables (features) \n",
        "in a learning problem. Reference Wikipedia.\n",
        "\n",
        "The model generated confidence score is the lowest \n",
        "among the models evaluated so far.\n",
        "'''\n",
        "\n",
        "# Gaussian Naive Bayes\n",
        "\n",
        "gaussian = GaussianNB()\n",
        "gaussian.fit(X_train, Y_train)\n",
        "Y_pred = gaussian.predict(X_test)\n",
        "acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n",
        "acc_gaussian\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_F-rR-34qqjh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "The perceptron is an algorithm for supervised learning \n",
        "of binary classifiers (functions that can decide whether \n",
        "an input, represented by a vector of numbers, belongs \n",
        "to some specific class or not). It is a type of linear \n",
        "classifier, i.e. a classification algorithm that makes \n",
        "its predictions based on a linear predictor function \n",
        "combining a set of weights with the feature vector. \n",
        "The algorithm allows for online learning, in that it \n",
        "processes elements in the training set one at a time. \n",
        "Reference Wikipedia.\n",
        "\n",
        "# Perceptron\n",
        "'''\n",
        "\n",
        "perceptron = Perceptron()\n",
        "perceptron.fit(X_train, Y_train)\n",
        "Y_pred = perceptron.predict(X_test)\n",
        "acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n",
        "acc_perceptron\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L1_x4y6-quTI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Linear SVC\n",
        "\n",
        "linear_svc = LinearSVC()\n",
        "linear_svc.fit(X_train, Y_train)\n",
        "Y_pred = linear_svc.predict(X_test)\n",
        "acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n",
        "acc_linear_svc\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wkoYadpaqz6a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Stochastic Gradient Descent\n",
        "\n",
        "sgd = SGDClassifier()\n",
        "sgd.fit(X_train, Y_train)\n",
        "Y_pred = sgd.predict(X_test)\n",
        "acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n",
        "acc_sgd\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6tUbacKIq3ZK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "This model uses a decision tree as a predictive model which \n",
        "maps features (tree branches) to conclusions about the target \n",
        "value (tree leaves). Tree models where the target variable \n",
        "can take a finite set of values are called classification \n",
        "trees; in these tree structures, leaves represent class labels \n",
        "and branches represent conjunctions of features that lead to \n",
        "those class labels. Decision trees where the target variable \n",
        "can take continuous values (typically real numbers) are \n",
        "called regression trees. Reference Wikipedia.\n",
        "\n",
        "The model confidence score is the highest among models \n",
        "evaluated so far.\n",
        "'''\n",
        "\n",
        "# Decision Tree\n",
        "\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "decision_tree.fit(X_train, Y_train)\n",
        "Y_pred = decision_tree.predict(X_test)\n",
        "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
        "acc_decision_tree\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "alLNsI0bq_GP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "The next model Random Forests is one of the most popular. \n",
        "Random forests or random decision forests are an ensemble \n",
        "learning method for classification, regression and other tasks, \n",
        "that operate by constructing a multitude of decision trees (n_estimators=100) \n",
        "at training time and outputting the class that is the mode \n",
        "of the classes (classification) or mean prediction (regression) \n",
        "of the individual trees. Reference Wikipedia.\n",
        "\n",
        "The model confidence score is the highest among models evaluated \n",
        "so far. We decide to use this model's output (Y_pred) for \n",
        "creating our competition submission of results.\n",
        "\n",
        "# Random Forest\n",
        "'''\n",
        "\n",
        "random_forest = RandomForestClassifier(n_estimators=100)\n",
        "random_forest.fit(X_train, Y_train)\n",
        "Y_pred = random_forest.predict(X_test)\n",
        "random_forest.score(X_train, Y_train)\n",
        "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
        "acc_random_forest\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QousS2EDrB-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Model evaluation\n",
        "\n",
        "'''\n",
        "We can now rank our evaluation of all the models to choose \n",
        "the best one for our problem. While both Decision Tree and \n",
        "Random Forest score the same, we choose to use Random Forest \n",
        "as they correct for decision trees' habit of overfitting to \n",
        "their training set.\n",
        "'''\n",
        "\n",
        "\n",
        "models = pd.DataFrame({\n",
        "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
        "              'Random Forest', 'Naive Bayes', 'Perceptron', \n",
        "              'Stochastic Gradient Decent', 'Linear SVC', \n",
        "              'Decision Tree'],\n",
        "    'Score': [acc_svc, acc_knn, acc_log, \n",
        "              acc_random_forest, acc_gaussian, acc_perceptron, \n",
        "              acc_sgd, acc_linear_svc, acc_decision_tree]})\n",
        "models.sort_values(by='Score', ascending=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}